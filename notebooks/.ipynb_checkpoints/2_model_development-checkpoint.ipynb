{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da86a85-53e6-41c0-a793-e8bd5ef95b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7246357-cc1a-420d-bd6f-40b5b8b57b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i just feel really helpless and heavy hearted</td>\n",
       "      <td>4</td>\n",
       "      <td>helpless heavy hearted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ive enjoyed being able to slouch about relax a...</td>\n",
       "      <td>0</td>\n",
       "      <td>enjoy able slouch relax unwind frankly week en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i gave up my internship with the dmrg and am f...</td>\n",
       "      <td>4</td>\n",
       "      <td>give internship dmrg distraught</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont know i feel so lost</td>\n",
       "      <td>0</td>\n",
       "      <td>not lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am a kindergarten teacher and i am thoroughl...</td>\n",
       "      <td>4</td>\n",
       "      <td>kindergarten teacher thoroughly weary job have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416804</th>\n",
       "      <td>i feel like telling these horny devils to find...</td>\n",
       "      <td>2</td>\n",
       "      <td>like tell horny devil find site suited sort in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416805</th>\n",
       "      <td>i began to realize that when i was feeling agi...</td>\n",
       "      <td>3</td>\n",
       "      <td>begin realize agitated restless thought dish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416806</th>\n",
       "      <td>i feel very curious be why previous early dawn...</td>\n",
       "      <td>5</td>\n",
       "      <td>curious previous early dawn not seek trouble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416807</th>\n",
       "      <td>i feel that becuase of the tyranical nature of...</td>\n",
       "      <td>3</td>\n",
       "      <td>becuase tyranical nature government el salvado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416808</th>\n",
       "      <td>i think that after i had spent some time inves...</td>\n",
       "      <td>5</td>\n",
       "      <td>spend investigate surrounding start curious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416543 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "0           i just feel really helpless and heavy hearted      4   \n",
       "1       ive enjoyed being able to slouch about relax a...      0   \n",
       "2       i gave up my internship with the dmrg and am f...      4   \n",
       "3                              i dont know i feel so lost      0   \n",
       "4       i am a kindergarten teacher and i am thoroughl...      4   \n",
       "...                                                   ...    ...   \n",
       "416804  i feel like telling these horny devils to find...      2   \n",
       "416805  i began to realize that when i was feeling agi...      3   \n",
       "416806  i feel very curious be why previous early dawn...      5   \n",
       "416807  i feel that becuase of the tyranical nature of...      3   \n",
       "416808  i think that after i had spent some time inves...      5   \n",
       "\n",
       "                                               clean_text  \n",
       "0                                  helpless heavy hearted  \n",
       "1       enjoy able slouch relax unwind frankly week en...  \n",
       "2                         give internship dmrg distraught  \n",
       "3                                                not lost  \n",
       "4       kindergarten teacher thoroughly weary job have...  \n",
       "...                                                   ...  \n",
       "416804  like tell horny devil find site suited sort in...  \n",
       "416805       begin realize agitated restless thought dish  \n",
       "416806       curious previous early dawn not seek trouble  \n",
       "416807  becuase tyranical nature government el salvado...  \n",
       "416808        spend investigate surrounding start curious  \n",
       "\n",
       "[416543 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('../data/emotions_clean.csv')\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56fc0b-f23e-4ab8-9685-79b7f2881e4f",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0413798-2930-4f37-9687-2f22e5b8fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split feature and target\n",
    "X = data['clean_text']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e2f1d01-5a04-4534-813a-a8c19b089fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram & Bigram\n",
    "vec_TF_IDF = TfidfVectorizer(ngram_range=(1, 2))\n",
    "x1_sparse = vec_TF_IDF.fit_transform(X)\n",
    "\n",
    "#Save vectorizer.vocabulary_\n",
    "pickle.dump(vec_TF_IDF,open(\"../models/feature_tf-idf.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b8c51-0304-4f66-bc88-d80ec407dab6",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc7d2be-4017-494d-bca4-50f9dfb2793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize distribution Chi-Square score based on feature rank\n",
    "scores, _ = chi2(x1_sparse, y)\n",
    "plt.plot(sorted(scores, reverse=True))\n",
    "plt.xlabel('Rank of Feature')\n",
    "plt.ylabel('Chi-Square Score')\n",
    "plt.title('Chi-Square Score Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f25fa4-7ea2-4b99-be03-302732e46a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select K best feature with Chi-Square\n",
    "chi2_features = SelectKBest(chi2, k=5000)\n",
    "X_kbest_features = chi2_features.fit_transform(x1_sparse, y)\n",
    "\n",
    "pickle.dump(chi2_features,open(\"../models/chi2_selector.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f72d1bc-96e2-4747-8edc-37c88b04d0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah fitur asli: 1227102\n",
      "Jumlah fitur terpilih: 5000\n"
     ]
    }
   ],
   "source": [
    "# save feature selected\n",
    "feature_names = vec_TF_IDF.get_feature_names_out()\n",
    "selected_features = feature_names[chi2_features.get_support()]\n",
    "\n",
    "print(\"Jumlah fitur asli:\", len(feature_names))\n",
    "print(\"Jumlah fitur terpilih:\", len(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63eb8d62-4631-480e-babb-ee07a66740e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abit funny</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely amazed</th>\n",
       "      <th>absolutely amazing</th>\n",
       "      <th>absolutely disgusted</th>\n",
       "      <th>absolutely enthralled</th>\n",
       "      <th>absolutely lovely</th>\n",
       "      <th>absolutely ludicrous</th>\n",
       "      <th>absolutely petrified</th>\n",
       "      <th>...</th>\n",
       "      <th>www</th>\n",
       "      <th>year</th>\n",
       "      <th>year amazed</th>\n",
       "      <th>year amazing</th>\n",
       "      <th>year nostalgic</th>\n",
       "      <th>year overwhelmed</th>\n",
       "      <th>year shocked</th>\n",
       "      <th>year strange</th>\n",
       "      <th>year surprised</th>\n",
       "      <th>yell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416538</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416539</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416540</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416541</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416542</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416543 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        abandon  abit funny      able  absolutely amazed  absolutely amazing  \\\n",
       "0           0.0         0.0  0.000000                0.0                 0.0   \n",
       "1           0.0         0.0  0.097200                0.0                 0.0   \n",
       "2           0.0         0.0  0.000000                0.0                 0.0   \n",
       "3           0.0         0.0  0.000000                0.0                 0.0   \n",
       "4           0.0         0.0  0.000000                0.0                 0.0   \n",
       "...         ...         ...       ...                ...                 ...   \n",
       "416538      0.0         0.0  0.000000                0.0                 0.0   \n",
       "416539      0.0         0.0  0.000000                0.0                 0.0   \n",
       "416540      0.0         0.0  0.000000                0.0                 0.0   \n",
       "416541      0.0         0.0  0.078996                0.0                 0.0   \n",
       "416542      0.0         0.0  0.000000                0.0                 0.0   \n",
       "\n",
       "        absolutely disgusted  absolutely enthralled  absolutely lovely  \\\n",
       "0                        0.0                    0.0                0.0   \n",
       "1                        0.0                    0.0                0.0   \n",
       "2                        0.0                    0.0                0.0   \n",
       "3                        0.0                    0.0                0.0   \n",
       "4                        0.0                    0.0                0.0   \n",
       "...                      ...                    ...                ...   \n",
       "416538                   0.0                    0.0                0.0   \n",
       "416539                   0.0                    0.0                0.0   \n",
       "416540                   0.0                    0.0                0.0   \n",
       "416541                   0.0                    0.0                0.0   \n",
       "416542                   0.0                    0.0                0.0   \n",
       "\n",
       "        absolutely ludicrous  absolutely petrified  ...  www  year  \\\n",
       "0                        0.0                   0.0  ...  0.0   0.0   \n",
       "1                        0.0                   0.0  ...  0.0   0.0   \n",
       "2                        0.0                   0.0  ...  0.0   0.0   \n",
       "3                        0.0                   0.0  ...  0.0   0.0   \n",
       "4                        0.0                   0.0  ...  0.0   0.0   \n",
       "...                      ...                   ...  ...  ...   ...   \n",
       "416538                   0.0                   0.0  ...  0.0   0.0   \n",
       "416539                   0.0                   0.0  ...  0.0   0.0   \n",
       "416540                   0.0                   0.0  ...  0.0   0.0   \n",
       "416541                   0.0                   0.0  ...  0.0   0.0   \n",
       "416542                   0.0                   0.0  ...  0.0   0.0   \n",
       "\n",
       "        year amazed  year amazing  year nostalgic  year overwhelmed  \\\n",
       "0               0.0           0.0             0.0               0.0   \n",
       "1               0.0           0.0             0.0               0.0   \n",
       "2               0.0           0.0             0.0               0.0   \n",
       "3               0.0           0.0             0.0               0.0   \n",
       "4               0.0           0.0             0.0               0.0   \n",
       "...             ...           ...             ...               ...   \n",
       "416538          0.0           0.0             0.0               0.0   \n",
       "416539          0.0           0.0             0.0               0.0   \n",
       "416540          0.0           0.0             0.0               0.0   \n",
       "416541          0.0           0.0             0.0               0.0   \n",
       "416542          0.0           0.0             0.0               0.0   \n",
       "\n",
       "        year shocked  year strange  year surprised  yell  \n",
       "0                0.0           0.0             0.0   0.0  \n",
       "1                0.0           0.0             0.0   0.0  \n",
       "2                0.0           0.0             0.0   0.0  \n",
       "3                0.0           0.0             0.0   0.0  \n",
       "4                0.0           0.0             0.0   0.0  \n",
       "...              ...           ...             ...   ...  \n",
       "416538           0.0           0.0             0.0   0.0  \n",
       "416539           0.0           0.0             0.0   0.0  \n",
       "416540           0.0           0.0             0.0   0.0  \n",
       "416541           0.0           0.0             0.0   0.0  \n",
       "416542           0.0           0.0             0.0   0.0  \n",
       "\n",
       "[416543 rows x 5000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization of selected features in a sparse matrix into a dataframe\n",
    "df_kbest_features = pd.DataFrame.sparse.from_spmatrix(\n",
    "    X_kbest_features,\n",
    "    columns=selected_features\n",
    ")\n",
    "\n",
    "df_kbest_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23633cd0-ac5c-4376-a98b-cba49ad4f520",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5335d3c5-e0f5-49d9-9f70-4fe8d378f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data train and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_kbest_features, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b93aa5b-7a10-4d11-8ea9-aea417e7a23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\USER\\Envs\\env_nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'C': 100, 'solver': 'lbfgs', 'class_weight': None, 'tol': 0.0001, 'max_iter': 1000, 'multi_class': 'multinomial', 'random_state': 42}\n",
      "Best Test Accuracy: 0.8953774502154629\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['saga','newton-cg','lbfgs'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'tol': [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "max_iter = 1000 \n",
    "multi_class = 'multinomial'\n",
    "random_state = 42\n",
    "\n",
    "best_params = None\n",
    "best_accuracy = 0\n",
    "\n",
    "results = []\n",
    "\n",
    "for C in param_grid['C']:\n",
    "    for solver in param_grid['solver']:\n",
    "        for class_weight in param_grid['class_weight']:\n",
    "            for tol in param_grid['tol']:\n",
    "                model = LogisticRegression(\n",
    "                    C=C, solver=solver, class_weight=class_weight, tol=tol, \n",
    "                    max_iter=max_iter, multi_class=multi_class, random_state=random_state\n",
    "                )\n",
    "                model.fit(X_train, y_train) \n",
    "                test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "                results.append({\n",
    "                    'C': C,\n",
    "                    'solver': solver,\n",
    "                    'class_weight': class_weight,\n",
    "                    'tol': tol,\n",
    "                    'max_iter': max_iter, \n",
    "                    'multi_class': multi_class, \n",
    "                    'random_state': random_state,\n",
    "                    'accuracy': test_accuracy,\n",
    "                })\n",
    "\n",
    "                if test_accuracy > best_accuracy:\n",
    "                    best_accuracy = test_accuracy\n",
    "                    best_model = model\n",
    "                    best_params = {'C': C, 'solver': solver, 'class_weight': class_weight, 'tol': tol,\n",
    "                                   'max_iter': max_iter, 'multi_class': multi_class, 'random_state': random_state}\n",
    "\n",
    "print(\"\\nBest Parameters:\", best_params)\n",
    "print(\"Best Test Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89b069fd-d692-478e-b1cd-b866c5f6e3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>solver</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>tol</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>multi_class</th>\n",
       "      <th>random_state</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>100.0</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.895377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>100.0</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.895377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>100.0</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.895377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>100.0</td>\n",
       "      <td>saga</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.895329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>100.0</td>\n",
       "      <td>saga</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>100.0</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>100.0</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>100.0</td>\n",
       "      <td>saga</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>100.0</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10.0</td>\n",
       "      <td>saga</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10.0</td>\n",
       "      <td>saga</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10.0</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10.0</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>10.0</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>10.0</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10.0</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>10.0</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10.0</td>\n",
       "      <td>saga</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.894201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>100.0</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.893325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>100.0</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>42</td>\n",
       "      <td>0.893325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C     solver class_weight     tol  max_iter  multi_class  \\\n",
       "68  100.0      lbfgs         None  0.0100      1000  multinomial   \n",
       "67  100.0      lbfgs         None  0.0010      1000  multinomial   \n",
       "66  100.0      lbfgs         None  0.0001      1000  multinomial   \n",
       "56  100.0       saga         None  0.0100      1000  multinomial   \n",
       "55  100.0       saga         None  0.0010      1000  multinomial   \n",
       "61  100.0  newton-cg         None  0.0010      1000  multinomial   \n",
       "62  100.0  newton-cg         None  0.0100      1000  multinomial   \n",
       "54  100.0       saga         None  0.0001      1000  multinomial   \n",
       "60  100.0  newton-cg         None  0.0001      1000  multinomial   \n",
       "37   10.0       saga         None  0.0010      1000  multinomial   \n",
       "36   10.0       saga         None  0.0001      1000  multinomial   \n",
       "43   10.0  newton-cg         None  0.0010      1000  multinomial   \n",
       "42   10.0  newton-cg         None  0.0001      1000  multinomial   \n",
       "44   10.0  newton-cg         None  0.0100      1000  multinomial   \n",
       "50   10.0      lbfgs         None  0.0100      1000  multinomial   \n",
       "49   10.0      lbfgs         None  0.0010      1000  multinomial   \n",
       "48   10.0      lbfgs         None  0.0001      1000  multinomial   \n",
       "38   10.0       saga         None  0.0100      1000  multinomial   \n",
       "63  100.0  newton-cg     balanced  0.0001      1000  multinomial   \n",
       "64  100.0  newton-cg     balanced  0.0010      1000  multinomial   \n",
       "\n",
       "    random_state  accuracy  \n",
       "68            42  0.895377  \n",
       "67            42  0.895377  \n",
       "66            42  0.895377  \n",
       "56            42  0.895329  \n",
       "55            42  0.894873  \n",
       "61            42  0.894861  \n",
       "62            42  0.894861  \n",
       "54            42  0.894861  \n",
       "60            42  0.894861  \n",
       "37            42  0.894321  \n",
       "36            42  0.894285  \n",
       "43            42  0.894273  \n",
       "42            42  0.894273  \n",
       "44            42  0.894261  \n",
       "50            42  0.894249  \n",
       "49            42  0.894249  \n",
       "48            42  0.894249  \n",
       "38            42  0.894201  \n",
       "63            42  0.893325  \n",
       "64            42  0.893325  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).sort_values('accuracy',ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3a36156-6564-4620-b239-197eaf5c846d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     24201\n",
      "           1       0.90      0.94      0.92     28201\n",
      "           2       0.83      0.73      0.78      6911\n",
      "           3       0.91      0.89      0.90     11461\n",
      "           4       0.86      0.85      0.86      9541\n",
      "           5       0.80      0.69      0.74      2994\n",
      "\n",
      "    accuracy                           0.90     83309\n",
      "   macro avg       0.87      0.84      0.85     83309\n",
      "weighted avg       0.89      0.90      0.89     83309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model valuation metrics\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "pickle.dump(best_model,open(\"../models/logistic_regression_model.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e65b69-6144-4d33-ba22-797b02a4f66c",
   "metadata": {},
   "source": [
    "The model demonstrates a strong overall performance, achieving an accuracy of 90% on the test set. The precision, recall, and F1-score suggest a balanced capability to identify all six emotion categories, with macro F1-score at 0.85 and weighted F1-score at 0.89. The joy and sadness categories perform the best, with F1-scores above 0.92, indicating their robust recognition. However, the love and surprise categories show lower F1-scores of 0.78 and 0.74, respectively, suggesting room for improvement in detecting these less frequent emotions.\n",
    "\n",
    "The macro-average scores highlight the model's ability to perform across all categories, while the weighted averages reflect a strong overall performance, considering class imbalances. The model’s strength in predicting more common emotions aligns well with its higher support for these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7023874-d2c2-4065-8586-d07d40d56732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold accuracy:  [0.89594161 0.89613367 0.89518539 0.89568954 0.89578557]\n",
      "avg accuracy:  0.8957471581701857\n"
     ]
    }
   ],
   "source": [
    "# Cross validation testing for best model \n",
    "cv        = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "fold_accuracy  = (cross_val_score(best_model,X_kbest_features,y,cv=cv,scoring='accuracy'))\n",
    "avg_accuracy = np.mean(fold_accuracy)\n",
    "print (\"fold accuracy: \", fold_accuracy)\n",
    "print (\"avg accuracy: \", avg_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70828855-5e81-442f-9e34-8d9c3a0bfa30",
   "metadata": {},
   "source": [
    "Cross-validation results demonstrate consistency in the model's performance, with fold accuracies ranging narrowly between 0.8952 and 0.8961, yielding an average accuracy of 89.57%. This consistency suggests the model generalizes well across different subsets of the dataset, indicating its robustness and reliability for emotion detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
